{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will with the help of the Medium Cookies we will sign-in the Medium.com site we will extarct the data from it and with the help of the Bs4 we will import BeautifulSoup with which we will make a Database.csv file that hold all the Data in it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the important needed stuff for the Extraction of Data from the Medium site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\") \n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Navigate to Medium\n",
    "driver.get(\"https://www.medium.com\")\n",
    "\n",
    "# Load cookies from JSON\n",
    "with open('cookies_medium_2.json', 'r') as file:\n",
    "    all_cookies = json.load(file)\n",
    "\n",
    "# Clear existing cookies\n",
    "driver.delete_all_cookies()\n",
    "\n",
    "# Add cookies\n",
    "for cookie in all_cookies:\n",
    "    if 'medium.com' in cookie.get('domain', ''):\n",
    "        driver.add_cookie(cookie)\n",
    "\n",
    "# Refresh page to apply cookies\n",
    "driver.get(\"https://medium.com\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Check if login was successful\n",
    "# print(driver.get_cookies())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_value=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[1]/div/div/input\")\n",
    "time.sleep(4)\n",
    "data_value.send_keys('Data Science')\n",
    "data_value.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current height be :933\n"
     ]
    }
   ],
   "source": [
    "height=driver.execute_script(' return document.body.scrollHeight')\n",
    "print(f'current height be :{height}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'/html/body/div[1]/div/div[3]/div[2]/div/main/div/div/div[2]/div/div[10]/div/div/button'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current Scroll\n",
      "New Scroll\n",
      "current Scroll\n",
      "New Scroll\n",
      "current Scroll\n",
      "New Scroll\n",
      "current Scroll\n",
      "New Scroll\n",
      "current Scroll\n",
      "New Scroll\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "\n",
    "  driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "  time.sleep(3)\n",
    "  print(f'current Scroll')\n",
    "\n",
    "  show_button=WebDriverWait(driver,10).until(\n",
    "    EC.element_to_be_clickable(\n",
    "      driver.find_element(By.XPATH,\"//button[contains(text(), 'Show more')]\")\n",
    "      ))\n",
    "  show_button.click()\n",
    "\n",
    "  time.sleep(3)\n",
    "  driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "  print(f'New Scroll')\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"n  p\"))\n",
    ")\n",
    "\n",
    "# Pass the updated page source to BeautifulSoup\n",
    "new_values = BeautifulSoup(driver.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InTowards Data SciencebyJason Tamara Widjaja\n",
      "InTowards Data SciencebyClaudia Ng\n",
      "InTowards Data SciencebyAnna Via\n",
      "InStackademicbyAbdur Rahman\n",
      "InTowards Data SciencebyEgor Howell\n",
      "InTowards Data SciencebyBenjamin Bodner\n",
      "InTowards Data SciencebySabrine Bendimerad\n",
      "Emmanuel Ikogho\n",
      "InTowards Data SciencebyJose Parre√±o\n",
      "InTowards Data SciencebyJose Parre√±o\n",
      "Harika Govada\n",
      "InTowards Data SciencebyDario Radeƒçiƒá\n",
      "InTowards Data SciencebyBenjamin Lee\n",
      "InTowards Data Sciencebyüí°Mike Shakhomirov\n",
      "InTowards AIbyJoseph Robinson, Ph.D.\n",
      "InLevel Up CodingbyJoseph Robinson, Ph.D.\n",
      "InTowards Data SciencebyDasha Herrmannova, Ph.D.\n",
      "InTowards Data SciencebyCol Jung\n",
      "InTowards Data SciencebyYu Dong\n",
      "InTowards Data SciencebyEgor Howell\n",
      "Bhavik Jikadara\n",
      "InTowards Data SciencebyKhouloud El Alami\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "Inbitgrit Data Science PublicationbyBenedict Neo\n",
      "InTowards Data SciencebyMatt Chapman\n",
      "Inbitgrit Data Science PublicationbyBenedict Neo\n",
      "InTowards Data SciencebySara N√≥brega\n",
      "InTowards Data SciencebyEgor Howell\n",
      "InTowards Data SciencebyEgor Howell\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "Aqsazafar\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "NYU Center for Data Science\n",
      "Mirko Peters\n",
      "NYU Center for Data Science\n",
      "ODSC - Open Data Science\n",
      "Zoumana Keita\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "ODSC - Open Data Science\n",
      "Titles count: 60\n",
      "Authors count: 60\n",
      "Titles: [['intowards data science'], ['intowards data science'], ['intowards data science'], ['instackademic'], ['intowards data science'], ['intowards data science'], ['intowards data science'], [], ['intowards data science'], ['intowards data science'], [], ['intowards data science'], ['intowards data science'], ['intowards data science'], ['intowards ai'], ['inlevel up coding'], ['intowards data science'], ['intowards data science'], ['intowards data science'], ['intowards data science'], [], ['intowards data science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['inbitgrit data science publication'], ['intowards data science'], ['inbitgrit data science publication'], ['intowards data science'], ['intowards data science'], ['intowards data science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], [], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['NYU Center for Data Science'], [], ['NYU Center for Data Science'], ['ODSC - Open Data Science'], [], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science'], ['ODSC - Open Data Science']]\n",
      "Authors: [['jason tamara widjaja'], ['claudia ng'], ['anna via'], ['abdur rahman'], ['egor howell'], ['benjamin bodner'], ['sabrine bendimerad'], ['Emmanuel Ikogho'], ['jose parre√±o'], ['jose parre√±o'], ['Harika Govada'], ['dario radeƒçiƒá'], ['benjamin lee'], ['üí°mike shakhomirov'], ['joseph robinson, ph.d.'], ['joseph robinson, ph.d.'], ['dasha herrmannova, ph.d.'], ['col jung'], ['yu dong'], ['egor howell'], ['Bhavik'], ['khouloud el alami'], [], [], ['benedict neo'], ['matt chapman'], ['benedict neo'], ['sara n√≥brega'], ['egor howell'], ['egor howell'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Mirko Peters'], [], [], ['Zoumana Keita'], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the smaller model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_function(split_data, author, publisher):\n",
    "    \"\"\"\n",
    "    Splits the data into publisher and author parts and appends them to the respective lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    publisher_part = split_data[0].strip() if split_data[0] else 'NaN'\n",
    "    author_part = split_data[1].strip() if split_data[1] else 'NaN'\n",
    "\n",
    "    if len(author_part.split()) == 1:\n",
    "        author.append(author_part)\n",
    "    else:\n",
    "        author.append(author_part)\n",
    "    publisher.append(publisher_part)\n",
    "    return author,publisher\n",
    "\n",
    "\n",
    "def single_word_handle(value_text, author, publisher):\n",
    "    \"\"\"\n",
    "    Handles cases where split data might not contain both publisher and author.\n",
    "    \"\"\"\n",
    "    doc = nlp(value_text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "\n",
    "            author.append(ent.text)\n",
    "            \n",
    "        else:\n",
    "            publisher.append(ent.text)\n",
    "    return author,publisher\n",
    "\n",
    "def all_names(div):\n",
    "    \"\"\"\n",
    "    Extracts titles and author names from a div.\n",
    "    \"\"\"\n",
    "    author = []\n",
    "    publisher = []\n",
    "\n",
    "    if div :\n",
    "        if 'by' in div.text.lower():\n",
    "            by_data = div.text.strip().lower()\n",
    "            split_data = re.split(r'\\s*by\\s*', by_data, maxsplit=1)\n",
    "            # lenght=len(split_data)\n",
    "            author, publisher = split_function(split_data, author, publisher)\n",
    "        else:\n",
    "            author,publisher=single_word_handle(div.text.strip(),author,publisher)\n",
    "    return author,publisher\n",
    "\n",
    "\n",
    "divs = new_values.find_all('div', class_='mt mu mv mw mx n o')\n",
    "for i in divs:\n",
    "    print(i.text)\n",
    "\n",
    "# Initialize lists to store results\n",
    "ti = []\n",
    "au = []\n",
    "\n",
    "# Process each div and collect titles and author names\n",
    "if divs:\n",
    "    for div in divs:\n",
    "        authors, publishers = all_names(div)\n",
    "        au.append(authors)\n",
    "        ti.append(publishers)\n",
    "\n",
    "# Print results\n",
    "print(\"Titles count:\", len(ti))\n",
    "print(\"Authors count:\", len(au))\n",
    "\n",
    "# Print individual results\n",
    "print(\"Titles:\", ti)\n",
    "print(\"Authors:\", au)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find all <div> elements with the specified class\n",
    "new_date = new_values.find_all('div', class_='n o gq')\n",
    "span_dates=[]\n",
    "\n",
    "# Extract text from <span> elements inside `new_date`\n",
    "for div in new_date[0::2]:\n",
    "    date_spans = div.find_all('span')  \n",
    "    for span in date_spans:\n",
    "        if span:\n",
    "            if '2024' in span.text: \n",
    "                span_dates.append(span.text.strip()) \n",
    "        else :\n",
    "            span_dates.append('None') \n",
    "\n",
    "\n",
    "# for i in span_dates:\n",
    "    # print(i)\n",
    "print(len(span_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "div_ = new_values.find_all('div', class_=\"cf ot dx n o\")\n",
    "\n",
    "if div_:\n",
    "    def cmt(div):\n",
    "        likes = div.find('span')\n",
    "        cmt = div.find_all('span')\n",
    "\n",
    "        total_likes = likes.text.strip() if likes else '0'\n",
    "        total_cmt = cmt[1].text.strip() if len(cmt)>1 else '0'\n",
    "        if total_likes:\n",
    "            if 'K' in total_likes:\n",
    "                dosy_data_without_k=total_likes.replace('K','')\n",
    "                if '.' in dosy_data_without_k:\n",
    "                    dosy_data_without_k=dosy_data_without_k.replace('.','')\n",
    "                    dosy_data_without_k+='00'\n",
    "                else:\n",
    "                    dosy_data_without_k+='000'\n",
    "            else:\n",
    "                dosy_data_without_k=total_likes\n",
    "            \n",
    "\n",
    "        return total_cmt,dosy_data_without_k\n",
    "    \n",
    "\n",
    "cmt_list =[]\n",
    "likes_list =[]\n",
    "\n",
    "for div in div_:\n",
    "    comment,likes = cmt(div)\n",
    "    cmt_list.append(comment)\n",
    "    likes_list.append(likes)\n",
    "        \n",
    "\n",
    "\n",
    "# print(len(likes_list))\n",
    "# print(len(cmt_list))\n",
    "\n",
    "new_list_likes=[]\n",
    "for i in likes_list[0::2]:\n",
    "    # print(i)\n",
    "    new_list_likes.append(i)\n",
    "new_list_cmt=[]\n",
    "for i in cmt_list[0::2]:\n",
    "    # print(i)\n",
    "    new_list_cmt.append(i)\n",
    "\n",
    "\n",
    "print(len(new_list_cmt))\n",
    "print(len(new_list_likes))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is Saved in the new csv file 'Extracted_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_list_au = [item[0] if item else 'NaN' for item in au ]\n",
    "new_list_ti = [items[0] if items else 'NaN' for items in ti ]\n",
    "\n",
    "data ={\n",
    "  'Author Name':new_list_au,\n",
    "  'Publisher':new_list_ti,\n",
    "  'Date':span_dates,\n",
    "  'Comment':new_list_cmt,\n",
    "  'Likes':new_list_likes\n",
    "  }\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "df.to_csv('Extracted_Data.csv',index = False)\n",
    "print(\"Data is Saved in the new csv file 'Extracted_Data.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
